#!/usr/bin/env python3
"""
ç®€åŒ–çš„å‚æ•°é‡å¯¹æ¯”æµ‹è¯•
"""

import torch
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from guided_diffusion.unet_hae import HAEUNetModel
from guided_diffusion.unet_hae_lite import HAEUNetModelLite

def count_parameters(model):
    """è®¡ç®—æ¨¡å‹å‚æ•°é‡"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return total_params, trainable_params

def count_transformer_params(model):
    """ç»Ÿè®¡Transformerç›¸å…³å‚æ•°"""
    transformer_params = 0
    projection_params = 0
    
    for name, module in model.named_modules():
        # ç»Ÿè®¡MultiScale Transformerå—
        if 'MultiScale' in type(module).__name__:
            module_params = sum(p.numel() for p in module.parameters())
            transformer_params += module_params
            
            # ç»Ÿè®¡æŠ•å½±å±‚å‚æ•°
            if hasattr(module, 'patch_projections'):  # åŸç‰ˆ
                for proj in module.patch_projections:
                    projection_params += sum(p.numel() for p in proj.parameters())
            elif hasattr(module, 'shared_patch_embed'):  # ç²¾ç®€ç‰ˆ
                projection_params += sum(p.numel() for p in module.shared_patch_embed.parameters())
    
    return transformer_params, projection_params

def main():
    print("HAE UNet å‚æ•°é‡ç²¾ç¡®å¯¹æ¯”")
    print("=" * 40)
    
    # æ¨¡å‹å‚æ•°
    model_args = {
        'image_size': 128,
        'in_channels': 4,
        'model_channels': 128,
        'out_channels': 4,
        'num_res_blocks': 2,
        'attention_resolutions': [32, 16, 8],
        'dropout': 0.1,
        'channel_mult': [1, 2, 4, 8],
        'num_classes': 2,
        'use_checkpoint': False,
        'num_heads': 1,
        'num_head_channels': -1,
        'use_scale_shift_norm': False,
        'resblock_updown': False,
        'use_new_attention_order': False,
        'clf_free': True,
        'use_hae': True
    }
    
    print("åˆ›å»ºæ¨¡å‹...")
    hae_model = HAEUNetModel(**model_args)
    hae_lite_model = HAEUNetModelLite(**model_args)
    
    # è®¡ç®—æ€»å‚æ•°é‡
    hae_total, _ = count_parameters(hae_model)
    hae_lite_total, _ = count_parameters(hae_lite_model)
    
    # è®¡ç®—Transformerå‚æ•°
    hae_trans_params, hae_proj_params = count_transformer_params(hae_model)
    hae_lite_trans_params, hae_lite_proj_params = count_transformer_params(hae_lite_model)
    
    print(f"\n=== æ€»å‚æ•°é‡å¯¹æ¯” ===")
    print(f"åŸç‰ˆHAE UNet: {hae_total:,} ({hae_total/1e6:.1f}M)")
    print(f"ç²¾ç®€ç‰ˆHAE UNet: {hae_lite_total:,} ({hae_lite_total/1e6:.1f}M)")
    
    reduction = hae_total - hae_lite_total
    reduction_ratio = reduction / hae_total * 100
    print(f"\nå‚æ•°å‡å°‘: {reduction:,} ({reduction/1e6:.1f}M)")
    print(f"å‡å°‘æ¯”ä¾‹: {reduction_ratio:.1f}%")
    print(f"å‹ç¼©æ¯”: {hae_total/hae_lite_total:.2f}x")
    
    print(f"\n=== Transformerå‚æ•°å¯¹æ¯” ===")
    print(f"åŸç‰ˆTransformerå‚æ•°: {hae_trans_params:,} ({hae_trans_params/1e6:.1f}M)")
    print(f"ç²¾ç®€ç‰ˆTransformerå‚æ•°: {hae_lite_trans_params:,} ({hae_lite_trans_params/1e6:.1f}M)")
    
    trans_reduction = hae_trans_params - hae_lite_trans_params
    print(f"Transformerå‚æ•°å‡å°‘: {trans_reduction:,} ({trans_reduction/1e6:.1f}M)")
    
    print(f"\n=== æŠ•å½±å±‚å‚æ•°å¯¹æ¯” ===")
    print(f"åŸç‰ˆæŠ•å½±å±‚å‚æ•°: {hae_proj_params:,} ({hae_proj_params/1e6:.1f}M)")
    print(f"ç²¾ç®€ç‰ˆæŠ•å½±å±‚å‚æ•°: {hae_lite_proj_params:,} ({hae_lite_proj_params/1e6:.1f}M)")
    
    proj_reduction = hae_proj_params - hae_lite_proj_params
    print(f"æŠ•å½±å±‚å‚æ•°å‡å°‘: {proj_reduction:,} ({proj_reduction/1e6:.1f}M)")
    print(f"æŠ•å½±å±‚å‡å°‘å æ€»å‡å°‘çš„æ¯”ä¾‹: {proj_reduction/reduction*100:.1f}%")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    print(f"\n=== åŠŸèƒ½éªŒè¯ ===")
    test_input = torch.randn(1, 4, 128, 128)
    test_timesteps = torch.randint(0, 1000, (1,))
    test_y = torch.randint(0, 2, (1,))
    
    try:
        with torch.no_grad():
            hae_output = hae_model(test_input, test_timesteps, test_y, clf_free=True)
            hae_lite_output = hae_lite_model(test_input, test_timesteps, test_y, clf_free=True)
            
        print(f"âœ… åŸç‰ˆè¾“å‡ºå½¢çŠ¶: {hae_output.shape}")
        print(f"âœ… ç²¾ç®€ç‰ˆè¾“å‡ºå½¢çŠ¶: {hae_lite_output.shape}")
        print(f"âœ… åŠŸèƒ½éªŒè¯é€šè¿‡ï¼")
        
        return True
        
    except Exception as e:
        print(f"âŒ åŠŸèƒ½éªŒè¯å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print("\nğŸ‰ ç²¾ç®€ç‰ˆHAE UNetå®ç°æˆåŠŸï¼")
        print("   - å¤§å¹…å‡å°‘äº†å‚æ•°é‡")
        print("   - ä¿æŒäº†æ¨¡å‹åŠŸèƒ½")
        print("   - å¯ä»¥æ­£å¸¸è¿›è¡Œè®­ç»ƒå’Œæ¨ç†")
    else:
        print("\nâŒ å®ç°å­˜åœ¨é—®é¢˜ï¼Œéœ€è¦æ£€æŸ¥ã€‚")